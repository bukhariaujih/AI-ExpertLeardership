{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module 1 - Real Time Facial Expression Recognition",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bukhariaujih/AI-ExpertLeardership/blob/master/Module_1_Real_Time_Facial_Expression_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qGDPKSCr6UD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "1fc254ae-1be3-4aaf-f012-a90f715bc9be"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from keras.preprocessing import image\n",
        "import time\n",
        "\n",
        "#-----------------------------\n",
        "#opencv initialization\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('C:/Users/IS96273/AppData/Local/Continuum/anaconda3/envs/tensorflow/Library/etc/haarcascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "#-----------------------------\n",
        "#face expression recognizer initialization\n",
        "from keras.models import model_from_json\n",
        "model = model_from_json(open(\"facial_expression_model_structure.json\", \"r\").read())\n",
        "model.load_weights('facial_expression_model_weights.h5') #load weights\n",
        "#-----------------------------"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3af3e8e507e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#face expression recognizer initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"facial_expression_model_structure.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'facial_expression_model_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#load weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#-----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'facial_expression_model_structure.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIMfoeiHuYg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wWkafnMuYgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "\n",
        "#cap = cv2.VideoCapture('zuckerberg.mp4') #process videos\n",
        "cap = cv2.VideoCapture(0) #process real time web-cam\n",
        "\n",
        "frame = 0\n",
        "\n",
        "while(True):\n",
        "\tret, img = cap.read()\n",
        "\t\n",
        "\timg = cv2.resize(img, (640, 360))\n",
        "\timg = img[0:308,:]\n",
        "\n",
        "\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\tfaces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "\tfor (x,y,w,h) in faces:\n",
        "\t\tif w > 130: #trick: ignore small faces\n",
        "\t\t\t#cv2.rectangle(img,(x,y),(x+w,y+h),(64,64,64),2) #highlight detected face\n",
        "\t\t\t\n",
        "\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
        "\t\t\tdetected_face = cv2.cvtColor(detected_face, cv2.COLOR_BGR2GRAY) #transform to gray scale\n",
        "\t\t\tdetected_face = cv2.resize(detected_face, (48, 48)) #resize to 48x48\n",
        "\t\t\t\n",
        "\t\t\timg_pixels = image.img_to_array(detected_face)\n",
        "\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\n",
        "\t\t\t\n",
        "\t\t\timg_pixels /= 255 #pixels are in scale of [0, 255]. normalize all pixels in scale of [0, 1]\n",
        "\t\t\t\n",
        "\t\t\t#------------------------------\n",
        "\t\t\t\n",
        "\t\t\tpredictions = model.predict(img_pixels) #store probabilities of 7 expressions\n",
        "\t\t\tmax_index = np.argmax(predictions[0])\n",
        "\t\t\t\n",
        "\t\t\t#background of expression list\n",
        "\t\t\toverlay = img.copy()\n",
        "\t\t\topacity = 0.4\n",
        "\t\t\tcv2.rectangle(img,(x+w+10,y-25),(x+w+150,y+115),(64,64,64),cv2.FILLED)\n",
        "\t\t\tcv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\n",
        "\t\t\t\n",
        "\t\t\t#connect face and expressions\n",
        "\t\t\tcv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),(255,255,255),1)\n",
        "\t\t\tcv2.line(img,(x+w,y-20),(x+w+10,y-20),(255,255,255),1)\n",
        "\t\t\t\n",
        "\t\t\temotion = \"\"\n",
        "\t\t\tfor i in range(len(predictions[0])):\n",
        "\t\t\t\temotion = \"%s %s%s\" % (emotions[i], round(predictions[0][i]*100, 2), '%')\n",
        "\t\t\t\t\n",
        "\t\t\t\t\"\"\"if i != max_index:\n",
        "\t\t\t\t\tcolor = (255,0,0)\"\"\"\n",
        "\t\t\t\t\t\n",
        "\t\t\t\tcolor = (255,255,255)\n",
        "\t\t\t\t\n",
        "\t\t\t\tcv2.putText(img, emotion, (int(x+w+15), int(y-12+i*20)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\t\t\t\t\n",
        "\t\t\t#-------------------------\n",
        "\t\n",
        "\tcv2.imshow('img',img)\n",
        "\t\n",
        "\tframe = frame + 1\n",
        "\t#print(frame)\n",
        "\t\n",
        "\t#---------------------------------\n",
        "\t\n",
        "\tif frame > 227:\n",
        "\t\tbreak\n",
        "\t\n",
        "\tif cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
        "\t\tbreak\n",
        "\n",
        "#kill open cv things\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# =========================\n",
        "\n",
        "# try:\n",
        "#   filename = take_photo()\n",
        "#   print('Saved to {}'.format(filename))\n",
        "  \n",
        "#   # Show the image which was just taken.\n",
        "#   display(Image(filename))\n",
        "# except Exception as err:\n",
        "#   # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "#   # grant the page permission to access it.\n",
        "#   print(str(err))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}