{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module 1 - Real Time Facial Expression Recognition",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bukhariaujih/AI-ExpertLeardership/blob/master/Module_1_Real_Time_Facial_Expression_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUdFUB5N1Nf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePBKNpzk1Nfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wWkafnMuYgj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "d3c21bc4-92ca-40e2-993a-94aef98f538c"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from keras.preprocessing import image\n",
        "\n",
        "#-----------------------------\n",
        "#opencv initialization\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('C:/ProgramData/Anaconda3/envs/tensorflow/Library/etc/haarcascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "#-----------------------------\n",
        "#face expression recognizer initialization\n",
        "from keras.models import model_from_json\n",
        "model = model_from_json(open(\"facial_expression_model_structure.json\", \"r\").read())\n",
        "model.load_weights('facial_expression_model_weights.h5') #load weights\n",
        "\n",
        "#-----------------------------\n",
        "\n",
        "emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "\n",
        "while(True):\n",
        "  #ret, img = cap.read()\n",
        "  img = Image(take_photo())\n",
        "\t#img = cv2.imread('C:/Users/IS96273/Desktop/hababam.jpg')\n",
        "  \n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "\t#print(faces) #locations of detected faces\n",
        "\n",
        "\tfor (x,y,w,h) in faces:\n",
        "\t\tcv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\n",
        "\t\t\n",
        "\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
        "\t\tdetected_face = cv2.cvtColor(detected_face, cv2.COLOR_BGR2GRAY) #transform to gray scale\n",
        "\t\tdetected_face = cv2.resize(detected_face, (48, 48)) #resize to 48x48\n",
        "\t\t\n",
        "\t\timg_pixels = image.img_to_array(detected_face)\n",
        "\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\n",
        "\t\t\n",
        "\t\timg_pixels /= 255 #pixels are in scale of [0, 255]. normalize all pixels in scale of [0, 1]\n",
        "\t\t\n",
        "\t\tpredictions = model.predict(img_pixels) #store probabilities of 7 expressions\n",
        "\t\t\n",
        "\t\t#find max indexed array 0: angry, 1:disgust, 2:fear, 3:happy, 4:sad, 5:surprise, 6:neutral\n",
        "\t\tmax_index = np.argmax(predictions[0])\n",
        "\t\t\n",
        "\t\temotion = emotions[max_index]\n",
        "\t\t\n",
        "\t\t#write emotion text above rectangle\n",
        "\t\tcv2.putText(img, emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
        "\t\t\n",
        "\t\t#process on detected face end\n",
        "\t\t#-------------------------\n",
        "\n",
        "\tcv2.imshow('img',img)\n",
        "\n",
        "\tif cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
        "\t\tbreak\n",
        "\n",
        "#kill open cv things\t\t\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-6cae21b82224>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pFMYkwO2wx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}